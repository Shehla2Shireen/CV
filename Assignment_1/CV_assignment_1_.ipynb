{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVsB5MRmx4Xu",
        "outputId": "d622b7f1-a073-474d-ce84-f2c4610b55e4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 107ms/step - accuracy: 0.3480 - loss: 1.7821 - val_accuracy: 0.5382 - val_loss: 1.2974\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 104ms/step - accuracy: 0.5647 - loss: 1.2323 - val_accuracy: 0.6066 - val_loss: 1.1314\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 91ms/step - accuracy: 0.6277 - loss: 1.0682 - val_accuracy: 0.6295 - val_loss: 1.0626\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 90ms/step - accuracy: 0.6664 - loss: 0.9673 - val_accuracy: 0.6697 - val_loss: 0.9600\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 89ms/step - accuracy: 0.6942 - loss: 0.8873 - val_accuracy: 0.6678 - val_loss: 0.9592\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 99ms/step - accuracy: 0.7077 - loss: 0.8454 - val_accuracy: 0.6784 - val_loss: 0.9311\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 109ms/step - accuracy: 0.7230 - loss: 0.8074 - val_accuracy: 0.6894 - val_loss: 0.9050\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 89ms/step - accuracy: 0.7394 - loss: 0.7582 - val_accuracy: 0.6838 - val_loss: 0.9308\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 86ms/step - accuracy: 0.7497 - loss: 0.7290 - val_accuracy: 0.6936 - val_loss: 0.9048\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 86ms/step - accuracy: 0.7596 - loss: 0.6959 - val_accuracy: 0.6932 - val_loss: 0.8875\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507ms/step - accuracy: 0.3339 - loss: 2.0720"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 534ms/step - accuracy: 0.3339 - loss: 2.0716 - val_accuracy: 0.4916 - val_loss: 1.5205 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543ms/step - accuracy: 0.4970 - loss: 1.3948"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 565ms/step - accuracy: 0.4970 - loss: 1.3947 - val_accuracy: 0.6134 - val_loss: 1.1064 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524ms/step - accuracy: 0.5685 - loss: 1.2104"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 547ms/step - accuracy: 0.5685 - loss: 1.2103 - val_accuracy: 0.6470 - val_loss: 1.0422 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.6095 - loss: 1.1051"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 524ms/step - accuracy: 0.6095 - loss: 1.1051 - val_accuracy: 0.6615 - val_loss: 1.0398 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501ms/step - accuracy: 0.6456 - loss: 1.0235"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 522ms/step - accuracy: 0.6456 - loss: 1.0235 - val_accuracy: 0.6938 - val_loss: 0.8920 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.6630 - loss: 0.9767"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 524ms/step - accuracy: 0.6630 - loss: 0.9767 - val_accuracy: 0.7096 - val_loss: 0.8671 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m 16/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:55\u001b[0m 543ms/step - accuracy: 0.6633 - loss: 1.0089"
          ]
        }
      ],
      "source": [
        "# CIFAR-10 Classification with Improved CNN\n",
        "\n",
        "# ## 1. Data Preparation\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load data\n",
        "#dataset: set of 60,000 color images (32x32 pixels) -> 10 classes\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()   #load data (already split into 4 parts - x means training images while y is categories for images)\n",
        "\n",
        "\n",
        "# Normalize -> change pxel values range -> 0-255 to 0-1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode\n",
        "'''\n",
        "Before :\n",
        "y_train = [6, 2, 3]\n",
        "\n",
        "After :\n",
        "each matix row -> a spf sample\n",
        "each matrix column -> a spf category\n",
        "y_train = [\n",
        "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],  # 6 → frog\n",
        "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],  # 2 → bird\n",
        "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   # 3 → cat\n",
        "]\n",
        "\n",
        "'''\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Optional: Augmentation\n",
        "# This creates an ImageDataGenerator object that will\n",
        "# randomly transform images during training to simulate variations and prevent overfitting.\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15, # Randomly rotates images by up to ±15 degrees\n",
        "    width_shift_range=0.1, # Randomly shifts images horizontally by up to 10% of image width\n",
        "    height_shift_range=0.1, # Randomly shifts images vertically by up to 10% of image height\n",
        "    horizontal_flip=True # Randomly flips images horizontally (left↔right)\n",
        ")\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# ## 2. Baseline Model\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "baseline_model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)), # Applies 32 filters of size 3x3 over the input image\n",
        "    MaxPooling2D((2,2)), # Downsamples the feature maps by taking the max value in 2x2 patches\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(), # Flattens the 2D feature maps into a 1D vector to feed into the Dense (fully connected) layers.\n",
        "    Dense(64, activation='relu'), # Fully connected layer with 64 neurons\n",
        "    Dense(10, activation='softmax') # layer with 10 units (since CIFAR-10 has 10 classes).\n",
        "                                    # softmax activation: outputs a probability distribution over the 10 classes.\n",
        "])\n",
        "\n",
        "baseline_model.compile(optimizer='adam',\n",
        "                       loss='categorical_crossentropy', # Since your labels are one-hot encoded, this is the correct loss function.\n",
        "                                                        # It measures the difference between the predicted probability distribution\n",
        "                                                        # (from softmax) and the actual distribution (one-hot vector).\n",
        "                       metrics=['accuracy'])            # percentage of correctly predicted classes.\n",
        "\n",
        "baseline_history = baseline_model.fit(x_train, y_train, # Your training data and labels (features + one-hot labels).\n",
        "                                      validation_data=(x_test, y_test), # After each epoch, the model evaluates its performance on test data to monitor generalization.\n",
        "                                      epochs=10, # The model will go through the entire training dataset 10 times.\n",
        "                                      batch_size=64) # each epoch will process 50,000 training samples but The data is processed in mini-batches of 64 samples at a time\n",
        "                                                     # 50000 / 64 ≈ 781.25 → rounded to 782 batches -> The model will run 782 mini-updates (batches) per epoch.\n",
        "\n",
        "# ## 3. Improved Model\n",
        "\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=10, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', save_best_only=True),\n",
        "    ReduceLROnPlateau(patience=5, factor=0.5)\n",
        "]\n",
        "\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=100,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "# ## 4. Evaluation\n",
        "\n",
        "# Plot accuracy/loss\n",
        "plt.plot(history.history['accuracy'], label='train acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy Curve\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix and report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred_classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcxFb4bV0A0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}